from llama_cpp import Llama
import json

# Inicia el model (nom茅s una vegada)
llm = Llama(model_path="./models/mistral-7b-instruct-v0.1.Q4_K_M.gguf", n_ctx=4096)

# Instrucci贸 base (el que tu li vols demanar sempre al model)
PROMPT_BASE = """
Extreu del text seg眉ent les dades en format JSON amb els camps seg眉ents:

- nom del projecte
- ubicaci贸
- pressupost de licitaci贸 (PEM)
- data de licitaci贸
- perfils t猫cnics requerits (amb titulaci贸, anys dexperi猫ncia i funcions)
- termini d'execuci贸
- requisits legals o t猫cnics destacats

Si alguna dada no hi 茅s, posa null. No afegeixis cap explicaci贸 ni cap text addicional.
"""

def processar_chunks(chunks):
    dades_totals = {
        "nom del projecte": None,
        "ubicaci贸": None,
        "pressupost de licitaci贸 (PEM)": None,
        "data de licitaci贸": None,
        "perfils t猫cnics requerits": [],
        "termini d'execuci贸": None,
        "requisits legals o t猫cnics destacats": None
    }

    for i, chunk in enumerate(chunks):
        prompt = f"{PROMPT_BASE}\n\nText:\n{chunk}"
        print(f" Processant chunk {i+1}/{len(chunks)}...")

        resposta = llm(prompt, max_tokens=800, stop=["</s>"], echo=False)
        try:
            json_resultat = json.loads(resposta["choices"][0]["text"].strip())

            # Fusionem la info
            for clau in dades_totals:
                if dades_totals[clau] is None and json_resultat.get(clau):
                    dades_totals[clau] = json_resultat[clau]
                elif isinstance(dades_totals[clau], list):
                    dades_totals[clau].extend(json_resultat.get(clau, []))

        except Exception as e:
            print(f"锔 Error en chunk {i+1}: {e}")
            continue

    return dades_totals
